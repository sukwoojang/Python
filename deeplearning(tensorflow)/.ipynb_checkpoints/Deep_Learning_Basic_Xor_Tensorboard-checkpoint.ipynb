{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Functions\n",
    "\n",
    "* 인간의 뇌구조와 유사하게 wx+b의 값이 일정 이상이면 1 이하이면 0으로 간주\n",
    "\n",
    "## Logistic regression units\n",
    "\n",
    "* softmax에서 다루었던 function -> 위의 Activation functions을 구현 가능하다.\n",
    "\n",
    "## False Promises\n",
    "\n",
    "* 컴퓨터가 자신을 인식할 수 있는 단계에 이를 수 있을거라는 58년도의 허황된 기사.\n",
    "* 간단하게 And/Or의 문제로 생각하기 시작(Linear로 구현 가능)\n",
    "* Xor문제에 의해서 깨지게 됨(Linear로 구현 불가, 구현 돼도 낮은 정확도)\n",
    "* (MLP:Multilayer perceptrons)를 통해서 Xor문제를 풀 수 있다, 단 학습 불가능\n",
    "\n",
    "## Backpropagation\n",
    "\n",
    "* 1974, 1982 Paul Werbos에 의해 발견, 후에 Hinton에 의해 재발견 -> CNN(알파고 적용)\n",
    "* layer수가 많아질수록 제대로 작동하지 않는 큰 문제가 발생(SVM,RF 등 ML 모델들이 부상)\n",
    "* 딥러닝의 두 번째 침체기\n",
    "\n",
    "##  Breakthrough\n",
    "\n",
    "* CIFAR의 지원을 통해 Hinton이 다시 한 번 딥러닝을 연구할 기회를 얻게 된다.\n",
    "* 초기값을 잘 선택하면 학습을 할 수 있다고 발표 (2006, 2007 상당히 최근)\n",
    "* 이미지 인식 정확도 2011년 25% 에러율 -> 2012년 15%, 2015년 5%미만 (획기적인 변화)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -0.74976003 [[-0.824055 ]\n",
      " [ 1.0770754]]\n",
      "100 -1.4350711 [[-0.44283152]\n",
      " [ 2.6037695 ]]\n",
      "200 -4.896902 [[1.9331421]\n",
      " [5.090199 ]]\n",
      "300 nan [[nan]\n",
      " [nan]]\n",
      "400 nan [[nan]\n",
      " [nan]]\n",
      "500 nan [[nan]\n",
      " [nan]]\n",
      "600 nan [[nan]\n",
      " [nan]]\n",
      "700 nan [[nan]\n",
      " [nan]]\n",
      "800 nan [[nan]\n",
      " [nan]]\n",
      "900 nan [[nan]\n",
      " [nan]]\n",
      "1000 nan [[nan]\n",
      " [nan]]\n",
      "1100 nan [[nan]\n",
      " [nan]]\n",
      "1200 nan [[nan]\n",
      " [nan]]\n",
      "1300 nan [[nan]\n",
      " [nan]]\n",
      "1400 nan [[nan]\n",
      " [nan]]\n",
      "1500 nan [[nan]\n",
      " [nan]]\n",
      "1600 nan [[nan]\n",
      " [nan]]\n",
      "1700 nan [[nan]\n",
      " [nan]]\n",
      "1800 nan [[nan]\n",
      " [nan]]\n",
      "1900 nan [[nan]\n",
      " [nan]]\n",
      "2000 nan [[nan]\n",
      " [nan]]\n",
      "2100 nan [[nan]\n",
      " [nan]]\n",
      "2200 nan [[nan]\n",
      " [nan]]\n",
      "2300 nan [[nan]\n",
      " [nan]]\n",
      "2400 nan [[nan]\n",
      " [nan]]\n",
      "2500 nan [[nan]\n",
      " [nan]]\n",
      "2600 nan [[nan]\n",
      " [nan]]\n",
      "2700 nan [[nan]\n",
      " [nan]]\n",
      "2800 nan [[nan]\n",
      " [nan]]\n",
      "2900 nan [[nan]\n",
      " [nan]]\n",
      "3000 nan [[nan]\n",
      " [nan]]\n",
      "3100 nan [[nan]\n",
      " [nan]]\n",
      "3200 nan [[nan]\n",
      " [nan]]\n",
      "3300 nan [[nan]\n",
      " [nan]]\n",
      "3400 nan [[nan]\n",
      " [nan]]\n",
      "3500 nan [[nan]\n",
      " [nan]]\n",
      "3600 nan [[nan]\n",
      " [nan]]\n",
      "3700 nan [[nan]\n",
      " [nan]]\n",
      "3800 nan [[nan]\n",
      " [nan]]\n",
      "3900 nan [[nan]\n",
      " [nan]]\n",
      "4000 nan [[nan]\n",
      " [nan]]\n",
      "4100 nan [[nan]\n",
      " [nan]]\n",
      "4200 nan [[nan]\n",
      " [nan]]\n",
      "4300 nan [[nan]\n",
      " [nan]]\n",
      "4400 nan [[nan]\n",
      " [nan]]\n",
      "4500 nan [[nan]\n",
      " [nan]]\n",
      "4600 nan [[nan]\n",
      " [nan]]\n",
      "4700 nan [[nan]\n",
      " [nan]]\n",
      "4800 nan [[nan]\n",
      " [nan]]\n",
      "4900 nan [[nan]\n",
      " [nan]]\n",
      "5000 nan [[nan]\n",
      " [nan]]\n",
      "5100 nan [[nan]\n",
      " [nan]]\n",
      "5200 nan [[nan]\n",
      " [nan]]\n",
      "5300 nan [[nan]\n",
      " [nan]]\n",
      "5400 nan [[nan]\n",
      " [nan]]\n",
      "5500 nan [[nan]\n",
      " [nan]]\n",
      "5600 nan [[nan]\n",
      " [nan]]\n",
      "5700 nan [[nan]\n",
      " [nan]]\n",
      "5800 nan [[nan]\n",
      " [nan]]\n",
      "5900 nan [[nan]\n",
      " [nan]]\n",
      "6000 nan [[nan]\n",
      " [nan]]\n",
      "6100 nan [[nan]\n",
      " [nan]]\n",
      "6200 nan [[nan]\n",
      " [nan]]\n",
      "6300 nan [[nan]\n",
      " [nan]]\n",
      "6400 nan [[nan]\n",
      " [nan]]\n",
      "6500 nan [[nan]\n",
      " [nan]]\n",
      "6600 nan [[nan]\n",
      " [nan]]\n",
      "6700 nan [[nan]\n",
      " [nan]]\n",
      "6800 nan [[nan]\n",
      " [nan]]\n",
      "6900 nan [[nan]\n",
      " [nan]]\n",
      "7000 nan [[nan]\n",
      " [nan]]\n",
      "7100 nan [[nan]\n",
      " [nan]]\n",
      "7200 nan [[nan]\n",
      " [nan]]\n",
      "7300 nan [[nan]\n",
      " [nan]]\n",
      "7400 nan [[nan]\n",
      " [nan]]\n",
      "7500 nan [[nan]\n",
      " [nan]]\n",
      "7600 nan [[nan]\n",
      " [nan]]\n",
      "7700 nan [[nan]\n",
      " [nan]]\n",
      "7800 nan [[nan]\n",
      " [nan]]\n",
      "7900 nan [[nan]\n",
      " [nan]]\n",
      "8000 nan [[nan]\n",
      " [nan]]\n",
      "8100 nan [[nan]\n",
      " [nan]]\n",
      "8200 nan [[nan]\n",
      " [nan]]\n",
      "8300 nan [[nan]\n",
      " [nan]]\n",
      "8400 nan [[nan]\n",
      " [nan]]\n",
      "8500 nan [[nan]\n",
      " [nan]]\n",
      "8600 nan [[nan]\n",
      " [nan]]\n",
      "8700 nan [[nan]\n",
      " [nan]]\n",
      "8800 nan [[nan]\n",
      " [nan]]\n",
      "8900 nan [[nan]\n",
      " [nan]]\n",
      "9000 nan [[nan]\n",
      " [nan]]\n",
      "9100 nan [[nan]\n",
      " [nan]]\n",
      "9200 nan [[nan]\n",
      " [nan]]\n",
      "9300 nan [[nan]\n",
      " [nan]]\n",
      "9400 nan [[nan]\n",
      " [nan]]\n",
      "9500 nan [[nan]\n",
      " [nan]]\n",
      "9600 nan [[nan]\n",
      " [nan]]\n",
      "9700 nan [[nan]\n",
      " [nan]]\n",
      "9800 nan [[nan]\n",
      " [nan]]\n",
      "9900 nan [[nan]\n",
      " [nan]]\n",
      "10000 nan [[nan]\n",
      " [nan]]\n",
      "\n",
      "Hypothesis :  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]] \n",
      "Correct:  [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]] \n",
      "Accuracy :  0.5\n"
     ]
    }
   ],
   "source": [
    "# logistic regression -> Xor\n",
    "x_data = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=np.float32)\n",
    "y_data = np.array([[0], [1], [1], [0]], dtype=np.float32)\n",
    "\n",
    "X = tf.placeholder(tf.float32)\n",
    "Y = tf.placeholder(tf.float32)\n",
    "W = tf.Variable(tf.random_normal([2, 1]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name = 'bias')\n",
    "\n",
    "hypothesis = tf.sigmoid(tf.matmul(X,W) + b)\n",
    "\n",
    "cost = tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype = tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(10001):\n",
    "        sess.run(train, feed_dict={X : x_data, Y : y_data})\n",
    "        if step % 100 == 0:\n",
    "            print(step, sess.run(cost, feed_dict={X:x_data, Y:y_data}), sess.run(W))\n",
    "            \n",
    "    h, c, a = sess.run([hypothesis, predicted, accuracy], feed_dict = {X : x_data, Y:y_data})\n",
    "    print(\"\\nHypothesis : \", h, \"\\nCorrect: \", c, \"\\nAccuracy : \", a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* logistic regreesion만으로는 결과가 좋지 않다.\n",
    "\n",
    "## Neural Net(NN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.72043234 [[ 0.022793 ]\n",
      " [-0.9107998]]\n",
      "100 0.6934269 [[ 0.022793 ]\n",
      " [-0.9107998]]\n",
      "200 0.6933812 [[ 0.022793 ]\n",
      " [-0.9107998]]\n",
      "300 0.6933429 [[ 0.022793 ]\n",
      " [-0.9107998]]\n",
      "400 0.69330823 [[ 0.022793 ]\n",
      " [-0.9107998]]\n",
      "500 0.69327646 [[ 0.022793 ]\n",
      " [-0.9107998]]\n",
      "600 0.69324684 [[ 0.022793 ]\n",
      " [-0.9107998]]\n",
      "700 0.69321877 [[ 0.022793 ]\n",
      " [-0.9107998]]\n",
      "800 0.6931916 [[ 0.022793 ]\n",
      " [-0.9107998]]\n",
      "900 0.69316494 [[ 0.022793 ]\n",
      " [-0.9107998]]\n",
      "1000 0.6931385 [[ 0.022793 ]\n",
      " [-0.9107998]]\n",
      "1100 0.69311166 [[ 0.022793 ]\n",
      " [-0.9107998]]\n",
      "1200 0.69308406 [[ 0.022793 ]\n",
      " [-0.9107998]]\n",
      "1300 0.69305545 [[ 0.022793 ]\n",
      " [-0.9107998]]\n",
      "1400 0.69302523 [[ 0.022793 ]\n",
      " [-0.9107998]]\n",
      "1500 0.692993 [[ 0.022793 ]\n",
      " [-0.9107998]]\n",
      "1600 0.69295824 [[ 0.022793 ]\n",
      " [-0.9107998]]\n",
      "1700 0.69292027 [[ 0.022793 ]\n",
      " [-0.9107998]]\n",
      "1800 0.6928785 [[ 0.022793 ]\n",
      " [-0.9107998]]\n",
      "1900 0.6928319 [[ 0.022793 ]\n",
      " [-0.9107998]]\n",
      "2000 0.6927796 [[ 0.022793 ]\n",
      " [-0.9107998]]\n",
      "2100 0.69272023 [[ 0.022793 ]\n",
      " [-0.9107998]]\n",
      "2200 0.6926521 [[ 0.022793 ]\n",
      " [-0.9107998]]\n",
      "2300 0.6925734 [[ 0.022793 ]\n",
      " [-0.9107998]]\n",
      "2400 0.69248146 [[ 0.022793 ]\n",
      " [-0.9107998]]\n",
      "2500 0.692373 [[ 0.022793 ]\n",
      " [-0.9107998]]\n",
      "2600 0.69224375 [[ 0.022793 ]\n",
      " [-0.9107998]]\n",
      "2700 0.69208825 [[ 0.022793 ]\n",
      " [-0.9107998]]\n",
      "2800 0.6918992 [[ 0.022793 ]\n",
      " [-0.9107998]]\n",
      "2900 0.69166696 [[ 0.022793 ]\n",
      " [-0.9107998]]\n",
      "3000 0.6913788 [[ 0.022793 ]\n",
      " [-0.9107998]]\n",
      "3100 0.6910169 [[ 0.022793 ]\n",
      " [-0.9107998]]\n",
      "3200 0.69055796 [[ 0.022793 ]\n",
      " [-0.9107998]]\n",
      "3300 0.6899695 [[ 0.022793 ]\n",
      " [-0.9107998]]\n",
      "3400 0.6892071 [[ 0.022793 ]\n",
      " [-0.9107998]]\n",
      "3500 0.6882099 [[ 0.022793 ]\n",
      " [-0.9107998]]\n",
      "3600 0.6868934 [[ 0.022793 ]\n",
      " [-0.9107998]]\n",
      "3700 0.68514186 [[ 0.022793 ]\n",
      " [-0.9107998]]\n",
      "3800 0.6827956 [[ 0.022793 ]\n",
      " [-0.9107998]]\n",
      "3900 0.67963547 [[ 0.022793 ]\n",
      " [-0.9107998]]\n",
      "4000 0.6753609 [[ 0.022793 ]\n",
      " [-0.9107998]]\n",
      "4100 0.66956127 [[ 0.022793 ]\n",
      " [-0.9107998]]\n",
      "4200 0.6616787 [[ 0.022793 ]\n",
      " [-0.9107998]]\n",
      "4300 0.6509662 [[ 0.022793 ]\n",
      " [-0.9107998]]\n",
      "4400 0.63645387 [[ 0.022793 ]\n",
      " [-0.9107998]]\n",
      "4500 0.61695546 [[ 0.022793 ]\n",
      " [-0.9107998]]\n",
      "4600 0.5911799 [[ 0.022793 ]\n",
      " [-0.9107998]]\n",
      "4700 0.55805695 [[ 0.022793 ]\n",
      " [-0.9107998]]\n",
      "4800 0.5173318 [[ 0.022793 ]\n",
      " [-0.9107998]]\n",
      "4900 0.47022456 [[ 0.022793 ]\n",
      " [-0.9107998]]\n",
      "5000 0.41957465 [[ 0.022793 ]\n",
      " [-0.9107998]]\n",
      "5100 0.36904964 [[ 0.022793 ]\n",
      " [-0.9107998]]\n",
      "5200 0.32187355 [[ 0.022793 ]\n",
      " [-0.9107998]]\n",
      "5300 0.27999294 [[ 0.022793 ]\n",
      " [-0.9107998]]\n",
      "5400 0.24404143 [[ 0.022793 ]\n",
      " [-0.9107998]]\n",
      "5500 0.21376568 [[ 0.022793 ]\n",
      " [-0.9107998]]\n",
      "5600 0.18848646 [[ 0.022793 ]\n",
      " [-0.9107998]]\n",
      "5700 0.16740984 [[ 0.022793 ]\n",
      " [-0.9107998]]\n",
      "5800 0.14978562 [[ 0.022793 ]\n",
      " [-0.9107998]]\n",
      "5900 0.13496786 [[ 0.022793 ]\n",
      " [-0.9107998]]\n",
      "6000 0.12242531 [[ 0.022793 ]\n",
      " [-0.9107998]]\n",
      "6100 0.11173087 [[ 0.022793 ]\n",
      " [-0.9107998]]\n",
      "6200 0.10254452 [[ 0.022793 ]\n",
      " [-0.9107998]]\n",
      "6300 0.09459621 [[ 0.022793 ]\n",
      " [-0.9107998]]\n",
      "6400 0.08767133 [[ 0.022793 ]\n",
      " [-0.9107998]]\n",
      "6500 0.081598364 [[ 0.022793 ]\n",
      " [-0.9107998]]\n",
      "6600 0.07623963 [[ 0.022793 ]\n",
      " [-0.9107998]]\n",
      "6700 0.07148394 [[ 0.022793 ]\n",
      " [-0.9107998]]\n",
      "6800 0.06724067 [[ 0.022793 ]\n",
      " [-0.9107998]]\n",
      "6900 0.06343569 [[ 0.022793 ]\n",
      " [-0.9107998]]\n",
      "7000 0.060007893 [[ 0.022793 ]\n",
      " [-0.9107998]]\n",
      "7100 0.056906458 [[ 0.022793 ]\n",
      " [-0.9107998]]\n",
      "7200 0.05408908 [[ 0.022793 ]\n",
      " [-0.9107998]]\n",
      "7300 0.051520094 [[ 0.022793 ]\n",
      " [-0.9107998]]\n",
      "7400 0.04916943 [[ 0.022793 ]\n",
      " [-0.9107998]]\n",
      "7500 0.047011446 [[ 0.022793 ]\n",
      " [-0.9107998]]\n",
      "7600 0.045024358 [[ 0.022793 ]\n",
      " [-0.9107998]]\n",
      "7700 0.04318933 [[ 0.022793 ]\n",
      " [-0.9107998]]\n",
      "7800 0.041490134 [[ 0.022793 ]\n",
      " [-0.9107998]]\n",
      "7900 0.039912794 [[ 0.022793 ]\n",
      " [-0.9107998]]\n",
      "8000 0.038445078 [[ 0.022793 ]\n",
      " [-0.9107998]]\n",
      "8100 0.037076265 [[ 0.022793 ]\n",
      " [-0.9107998]]\n",
      "8200 0.03579701 [[ 0.022793 ]\n",
      " [-0.9107998]]\n",
      "8300 0.0345991 [[ 0.022793 ]\n",
      " [-0.9107998]]\n",
      "8400 0.033475168 [[ 0.022793 ]\n",
      " [-0.9107998]]\n",
      "8500 0.032418847 [[ 0.022793 ]\n",
      " [-0.9107998]]\n",
      "8600 0.031424295 [[ 0.022793 ]\n",
      " [-0.9107998]]\n",
      "8700 0.030486465 [[ 0.022793 ]\n",
      " [-0.9107998]]\n",
      "8800 0.029600702 [[ 0.022793 ]\n",
      " [-0.9107998]]\n",
      "8900 0.028762985 [[ 0.022793 ]\n",
      " [-0.9107998]]\n",
      "9000 0.027969485 [[ 0.022793 ]\n",
      " [-0.9107998]]\n",
      "9100 0.027216982 [[ 0.022793 ]\n",
      " [-0.9107998]]\n",
      "9200 0.02650237 [[ 0.022793 ]\n",
      " [-0.9107998]]\n",
      "9300 0.02582301 [[ 0.022793 ]\n",
      " [-0.9107998]]\n",
      "9400 0.025176352 [[ 0.022793 ]\n",
      " [-0.9107998]]\n",
      "9500 0.024560144 [[ 0.022793 ]\n",
      " [-0.9107998]]\n",
      "9600 0.023972366 [[ 0.022793 ]\n",
      " [-0.9107998]]\n",
      "9700 0.023411069 [[ 0.022793 ]\n",
      " [-0.9107998]]\n",
      "9800 0.022874594 [[ 0.022793 ]\n",
      " [-0.9107998]]\n",
      "9900 0.02236137 [[ 0.022793 ]\n",
      " [-0.9107998]]\n",
      "10000 0.02186992 [[ 0.022793 ]\n",
      " [-0.9107998]]\n",
      "\n",
      "Hypothesis :  [[0.02127078]\n",
      " [0.9803796 ]\n",
      " [0.9722953 ]\n",
      " [0.017906  ]] \n",
      "Correct:  [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]] \n",
      "Accuracy :  1.0\n"
     ]
    }
   ],
   "source": [
    "x_data = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=np.float32)\n",
    "y_data = np.array([[0], [1], [1], [0]], dtype=np.float32)\n",
    "\n",
    "X = tf.placeholder(tf.float32)\n",
    "Y = tf.placeholder(tf.float32)\n",
    "\n",
    "W1 = tf.Variable(tf.random_normal([2, 2]), name='weight1') \n",
    "#열 값을 증가시키면 network가 wide해짐\n",
    "b1 = tf.Variable(tf.random_normal([2]), name='bias1')\n",
    "layer1 = tf.sigmoid(tf.matmul(X,W1) + b1)\n",
    "#layer를 증가시키면 network가 deep해짐\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([2, 1]), name='weight2')\n",
    "b2 = tf.Variable(tf.random_normal([1]), name='bias2')\n",
    "hypothesis = tf.sigmoid(tf.matmul(layer1,W2) + b2)\n",
    "\n",
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype = tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(10001):\n",
    "        sess.run(train, feed_dict={X : x_data, Y : y_data})\n",
    "        if step % 100 == 0:\n",
    "            print(step, sess.run(cost, feed_dict={X:x_data, Y:y_data}), sess.run(W))\n",
    "            \n",
    "    h, c, a = sess.run([hypothesis, predicted, accuracy], feed_dict = {X : x_data, Y:y_data})\n",
    "    print(\"\\nHypothesis : \", h, \"\\nCorrect: \", c, \"\\nAccuracy : \", a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Attempted to use a closed Session.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-504fbd02091e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;31m# Run summary merge and add summary\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mx_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0my_data\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[0mwriter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_summary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\tjrdn\\miniconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    948\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 950\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    951\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\tjrdn\\miniconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1094\u001b[0m     \u001b[1;31m# Check session.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1095\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_closed\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1096\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Attempted to use a closed Session.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1097\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mversion\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1098\u001b[0m       raise RuntimeError('The Session graph is empty.  Add operations to the '\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Attempted to use a closed Session."
     ]
    }
   ],
   "source": [
    "# From Tf graph, decide which tensors you want to log\n",
    "w2_hist = tf.summary.histogram(\"weights2\", W2)\n",
    "cost_sum = tf.summary.scalar(\"cost\", cost)\n",
    "\n",
    "# Merge all summaries\n",
    "summary = tf.summary.merge_all()\n",
    "\n",
    "# Create writer and add graph\n",
    "writer = tf.summary.FileWriter('./logs')\n",
    "writer.add_graph(sess.graph)\n",
    "\n",
    "# Run summary merge and add summary\n",
    "s, _ = sess.run([summary, train], feed_dict = {X:x_data,Y:y_data})\n",
    "writer.add_summary(s, global_step=global_step)\n",
    "\n",
    "# Launch TensorBoard\n",
    "# tensorboard --logdir=./logs (on cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0723 20:22:07.988554 13732 deprecation.py:323] From <ipython-input-10-5c81c5f4c9da>:3: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "W0723 20:22:07.989552 13732 deprecation.py:323] From c:\\users\\tjrdn\\miniconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "W0723 20:22:07.991547 13732 deprecation.py:323] From c:\\users\\tjrdn\\miniconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use urllib or similar directly.\n",
      "W0723 20:22:09.155433 13732 deprecation.py:323] From c:\\users\\tjrdn\\miniconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0723 20:22:09.627171 13732 deprecation.py:323] From c:\\users\\tjrdn\\miniconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "W0723 20:22:09.631161 13732 deprecation.py:323] From c:\\users\\tjrdn\\miniconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0723 20:22:10.683347 13732 deprecation.py:323] From c:\\users\\tjrdn\\miniconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = mnist.train\n",
    "test_data = mnist.test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 nan [[nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " ...\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]]\n",
      "100 nan [[nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " ...\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]]\n",
      "200 nan [[nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " ...\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-f7c6aeb14552>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10001\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mX\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m100\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\tjrdn\\miniconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    948\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 950\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    951\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\tjrdn\\miniconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1171\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1172\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1173\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1174\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1175\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\tjrdn\\miniconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1348\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1349\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1350\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1351\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1352\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\tjrdn\\miniconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1354\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1355\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1356\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1357\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1358\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\tjrdn\\miniconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1339\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1340\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1341\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1342\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1343\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\tjrdn\\miniconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1427\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1428\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1429\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1430\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1431\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "X = tf.placeholder(tf.float32,[None, 784])\n",
    "Y = tf.placeholder(tf.float32,[None, 10])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([784, 10]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([10]), name='bias')\n",
    "\n",
    "hypothesis = tf.sigmoid(tf.matmul(X,W) + b)\n",
    "\n",
    "cost = tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype = tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(10001):\n",
    "        sess.run(train, feed_dict={X : train_data.images, Y : train_data.labels})\n",
    "        if step % 100 == 0:\n",
    "            print(step, sess.run(cost, feed_dict={X:train_data.images, Y:train_data.labels}), sess.run(W))\n",
    "            \n",
    "    h, c, a = sess.run([hypothesis, predicted, accuracy], feed_dict = {X : test_data.images, Y:test_data.labels})\n",
    "    print(\"\\nHypothesis : \", h, \"\\nCorrect: \", c, \"\\nAccuracy : \", a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
